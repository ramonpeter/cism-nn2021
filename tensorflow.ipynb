{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_This notebook is part of the material for the [\"An Introduction to Neural Networks\"](https://indico.cism.ucl.ac.be/event/106/) session of the [2021 CISM/CEÃÅCI trainings](https://indico.cism.ucl.ac.be/category/6/)._\n",
    "\n",
    "[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ramonpeter/cism-nn2021/blob/main/tensorflow.ipynb)\n",
    "\n",
    "# Basics of Tensorflow\n",
    "\n",
    "Tensorflow is a widely used library for machine learning, especially deep learning, both training and inference (evaluating trained neural networks on new data).\n",
    "It was developed by the Google Brain team, and is open source software.\n",
    "On the [website](https://www.tensorflow.org) you will find many libraries and tools for common tasks related to machine learning, and a lot of training material and examples.\n",
    "\n",
    "In this first part, we will get familiar with basic concepts of [Tensorflow](https://www.tensorflow.org).\n",
    "\n",
    "## Fundamental classes and concepts\n",
    "\n",
    "The  `Tensor` class can be used for many things be just like a [numpy](https://numpy.org) array (see [this guide](https://www.tensorflow.org/guide/tensor) for some more advanced uses)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "x1 = tf.linspace(0., 1., 11)\n",
    "print(x1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To construct a `Tensor` from its value(s), the `tf.constant` helper method can be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = tf.constant(3.14)\n",
    "x2 = tf.constant([ [ 1., 2.], [3., 4. ] ])\n",
    "print(x1)\n",
    "print(x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the default type is a 32-bit floating point number. We can also construct a 32-bit integer type `Tensor`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x3 = tf.constant([ 1, 1, 2, 3, 5, 8, 13, 25 ])\n",
    "print(x3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are all tensors with a static shape, like in numpy, but it is also possible to make tensors with dynamic shape, if the size in one dimension is not known beforehand.\n",
    "This is used for input nodes, which can be constructed before the batch size is known:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x4 = tf.keras.layers.Input(shape=(3,))\n",
    "print(x4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to make tensors where one dimension changes from entry to entry, and to place a `Tensor` on a GPU.\n",
    "\n",
    "For weights in the neural network, the `Variable` class should be used, which is essentially a `Tensor` with extra functionality: the values can be stored to and loaded from a file, and it is possible to calculate derivatives with respect to a `Variable`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic differentiation\n",
    "\n",
    "Calculating the gradient of the neural network with respect to one of its parameters is called [backpropagation](https://en.wikipedia.org/wiki/Backpropagation).\n",
    "It is a very efficient way to calculate all the derivatives, since each derivative can easily be computed from the weights and the values of each node.\n",
    "\n",
    "Tensorflow implements [automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation) (a more detailed description can be found on [this documentation page](https://www.tensorflow.org/guide/autodiff)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(2.0) #needs to be float\n",
    "with tf.GradientTape() as tape:\n",
    "    y = x**2\n",
    "dydx = tape.gradient(y, x)\n",
    "print(dydx.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned before, trainable parameters should be a `Variable` instances, because computation graphs can change the values of a `Variable`, but not of a `Tensor` (this allows to make a single graph for a training step, or even for the whole training of a neural network)."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fbdf411563de2940c19fcb2473c8e49d19534f4d2fcc91fb3003c5547f3eefd7"
  },
  "kernelspec": {
   "display_name": "ceci_mltf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "nikola": {
   "author": "Pieter David",
   "category": "",
   "date": "2020-11-03 20:44:27 UTC+01:00",
   "description": "",
   "link": "",
   "pretty_url": "False",
   "slug": "tfprimer",
   "tags": "",
   "title": "A flavour of Tensorflow",
   "type": "text"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
